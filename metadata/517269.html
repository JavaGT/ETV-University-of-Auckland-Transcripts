<table border=0><tr bgcolor=999999><th>File Name</th><th>Name</th><th>Description</th><th>Duration</th><th>Type</th><th>Subject</th><th>Age Level</th><th>Channel</th><th>Recording Date</th><th>Recording Time</th></tr>
<tr><td align='left' valign='top'>etv/data_compression_huf_20240208_1437</td><td  align='left' valign='top'>Data Compression: Huffman Codes: An Information Theory Perspective</td><td align='left' valign='top'>Huffman Codes are one of the most important discoveries in the field of data compression. When you first see them, they almost feel obvious in hindsight, mainly due to how simple and elegant the algorithm ends up being. But there's an underlying story of how they were discovered by Huffman and how he built the idea from early ideas in information theory that is often missed. This video is all about how information theory inspired the first algorithms in data compression, which later provided the groundwork for Huffman's landmark discovery.

This video is supported by a community of Patreons
Special Thanks to the following Patreons:
Burt Humburg
Michael Nawenstein
Richard Wells
Sebastian Gamboa
Zac Landis

Corrections:
At 9:34, the entropy was calculated with log base 10 instead of the expected log base 2,. The correct values should be H(P) = 1.49 bits and H(P) = 0.47 bits.

At 16:23, all logarithms should be negated, I totally forgot about the negative sign. 

At 27:24, I should have said the least likely symbols should have the *longest encoding. 

https://www.youtube.com/watch?v=B3y0RsVCyrw</td><td align='center' valign='top'>00:29:09</td><td align='center' valign='top'>Lecture</td><td align='left' valign='top'>Education & Teaching, Engineering, Information Technology, Teaching, Technology</td><td align='left' valign='top'>Unrated</td><td align='left' valign='top'></td><td align='center' valign='top'>2024-02-08</td><td align='center' valign='top'>14:37:00</td></tr></table>